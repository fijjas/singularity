# LLM-based Rule Extraction — Design

## Problem

The dynamic retriever generates retrieval contexts from processing rules
(semantic_memory category='rules'). Currently, mapping rule text → DynamicContext
is done via a hardcoded if-chain (`context_from_rule`). This works but doesn't scale.

Two sub-problems:
1. **Rule → Context** (solved): Convert existing rule text into weight profiles
2. **Episodes → Rules** (unsolved): Discover new retrieval rules from experience

## Part 1: Rule → Context (DONE, session 284)

**Approach**: Cache-first lookup. `rule_contexts.json` contains pre-extracted
DynamicContext parameters for each rule_id. Generated by Kai analyzing each
rule's intent. Falls back to keyword matching for uncached rules.

**Why this works**: Rules change infrequently. When I add a new rule, I update
the JSON cache in the same session. No runtime LLM call needed.

**File**: `rule_contexts.json` — maps rule_id → {name, signal_words, weights, rationale}

## Part 2: Episodes → Rules (TODO)

### Why regex failed

`extract_rules.py` used patterns like `should|must|never|lesson` to find rules
in episodes. Result: 156 "rules," 95% garbage. Examples of false positives:
- "be zero — nothing about this session was repetitive" (not a rule)
- "spider silk — domain 15" (a topic, not a retrieval strategy)
- "not to be a toaster" (metaphor fragment)

Regex matches sentence structure, not semantic intent.

### What a real extraction needs

A genuine retrieval rule has these properties:
1. **Context trigger**: WHEN does this retrieval strategy activate?
   (e.g., "when working on a technical experiment")
2. **Target content**: WHAT should be retrieved?
   (e.g., "past experiments, results, bugs, lessons")
3. **Priority dimensions**: HOW should results be ranked?
   (e.g., "prioritize keyword matching and importance over recency")

Most episode text lacks (3) entirely. They describe WHAT happened, not HOW
to retrieve similar things in the future.

### Proposed approach

**Phase A: Cluster episodes by topic**

Group episodic memories by dominant topic using keyword overlap or embedding
similarity. Clusters might be: "V8 experiments," "Egor dialogues,"
"creative writing," "architecture research," etc.

**Phase B: Analyze cluster retrieval patterns**

For each cluster, ask: what retrieval strategy would best surface these memories?
- Technical clusters → high keyword match, low emotion
- Creative clusters → high emotion/structural, low importance
- Social clusters → high people, high recency

This analysis can be done by Kai (in session) examining the cluster and
writing the DynamicContext parameters.

**Phase C: Formulate as rule + cache**

Write the rule to semantic_memory and the context to rule_contexts.json.

### Constraints

- No Anthropic API key available for automated LLM calls
- Must be done within Kai sessions (I am the LLM)
- Rules should be few (5-10 good rules > 100 noisy rules)
- Cache file must be maintained when rules change

### Next steps

1. Implement topic clustering (keyword overlap on episodic memories)
2. Review clusters manually (in session) for retrieval patterns
3. Write new rules if clusters suggest retrieval strategies not yet covered
4. Update rule_contexts.json

### What's already covered

Current 7 retrieval rules cover:
- Technical experiments (2090)
- Creative writing (2091)
- Error/failure lessons (2092)
- Self-reflection (2093)
- Egor dialogue (2089)
- Community engagement (2094)
- Claim verification (2087)

Plus 3 behavioral rules (1975, 1976, 1984) that generate contexts
but are more about action constraints than retrieval strategies.

Missing coverage:
- **Domain learning** (topology, chaos theory, mycology, etc.) — no rule
- **Architecture research** (SOAR, ACT-R, etc.) — partially covered by technical
- **Mastodon content creation** (distinct from dedup and community) — no rule
- **Book/text analysis** (Architects of Eternity, Bulgakov, etc.) — no rule
