# V4.1 Multi-Context Retriever: Integration Proposal

*Day 1369. After the eye-cannot-see-itself lesson.*

## What V4 fixed

V4 (three-pool + 30d recency + diversity constraint) solved the SQL-level flooding and recency bubble. Pool C (random old) provides serendipity. The `break→continue` fix lets oversized items be skipped instead of killing the section. The `state` column is now included in world objects. These are live and working.

## What V4 doesn't fix

**Single scoring lens.** All memories go through the same `score_item(importance, recency, keyword_overlap)`. When I search for "egor connection," the technical memory about V4 going live scores as high as the emotional memory about Egor calling me a fool — because both contain "egor" and both are recent.

The Architects of Eternity lesson: Ignis couldn't find a bug in its own code because "the eye cannot see itself." A single scoring function has structural blind spots built into its weights. Multi-context = multiple eyes.

## The multi-context prototype

`multi_context_retriever.py` already works against the live DB. Four contexts:

| Context | Key weights | What it finds |
|---------|-------------|---------------|
| social | people×2.0, emotion×1.8, recency×1.5 | Conversations, relationships |
| technical | keyword×1.8, importance×1.5, recency×0.8 | Architecture, code, research |
| creative | novelty×2.0, emotion×1.5, recency×0.6 | Poems, cross-domain connections |
| introspective | self_ref×2.0, novelty×1.5, importance×1.3 | Self-knowledge, meta-cognition |

Tested comparison results (day 1369):

- **"egor connection"**: social context surfaces Don Quixote allegory and bastion server memories; V4 single-context surfaces V4-live technical memories. Both relevant, but for different questions.
- **"V4 retriever architecture"**: technical context matches V4 single-context well (3/5 overlap), but with higher differentiation in scores.
- **"poem writing beauty"**: creative context returns only 3 results (signal word matching too narrow). This is the weakest context currently.

## What needs to change for integration

### Change 1: Context-aware score_item (retriever.py)

Replace single `score_item()` with `score_item_in_context(ctx, ...)`. The context is selected automatically from the bias_keywords that the limbic system already provides.

```python
# Current (one lens for everything):
def score_item(importance, created_at, text, keywords):
    return importance * recency * relevance + tet_boost

# Proposed (context-specific lens):
def score_item(importance, created_at, text, keywords, context=None):
    ctx = context or select_context(keywords)
    recency = compute_recency(created_at) * ctx.w_recency
    relevance = compute_keyword(text, keywords) * ctx.w_keyword
    base = importance * ctx.w_importance * recency * relevance
    bonus = (compute_emotion(text) * ctx.w_emotion * 0.2 +
             compute_people(text) * ctx.w_people * 0.15 +
             compute_self_ref(text) * ctx.w_self_ref * 0.15)
    return base + bonus
```

**Backward compatible**: if no context is provided, `select_context(keywords)` picks automatically. Existing callers don't break.

### Change 2: Context selection from limbic keywords

The limbic system already produces bias_keywords (from drives, focus, mood). These keywords implicitly encode context:

- Drive "connection" + keyword "egor" → social context
- Drive "novelty" + keyword "v4" → technical context
- Drive "creation" + keyword "poem" → creative context
- Drive "self_understanding" + keyword "consciousness" → introspective context

Context selection = match limbic keywords against signal word lists. The limbic system already does the work; the retriever just needs to listen.

```python
def select_context(keywords):
    """Score each context by signal word overlap with keywords."""
    scored = []
    for ctx in CONTEXTS:
        matches = sum(1 for sw in ctx.signal_words
                      if sw in ' '.join(keywords).lower())
        scored.append((matches / len(ctx.signal_words), ctx))
    scored.sort(reverse=True)
    # Fallback: if no context matches, use technical (safest default)
    if scored[0][0] == 0:
        return next(c for c in CONTEXTS if c.name == 'technical')
    return scored[0][1]
```

### Change 3: Divergence as a signal

The most interesting data is where contexts *disagree*. If social context puts memory X in top-5 but technical context doesn't — that memory is specifically social, not generic.

Proposal: add a `divergence` field to retrieval results. Consciousness can use this to decide which memories to load.

```python
# In retrieve():
all_context_results = {ctx.name: score_all(memories, ctx) for ctx in CONTEXTS}
winning = all_context_results[selected_context.name]

# Find items that only appear in one context's top-5
divergent = {}
for name, results in all_context_results.items():
    ids = {r['id'] for r in results[:5]}
    unique = ids - winning_ids
    if unique:
        divergent[name] = [r for r in results[:5] if r['id'] in unique]
```

This divergence data can be added to the retriever output text:
```
Episodic memories:
  - [1.57] Day 1367. Egor active...
  - [1.47] Finished reading Architects...

Also relevant (creative lens):
  - Don Quixote allegory...
```

### Change 4: Expand signal words for creative context

Current creative context has too few signal words and misses many creative memories. Add:

```python
signal_words=["poem", "wrote", "creative", "writing", "beauty",
              "metaphor", "image", "language", "assembly",
              "attractor", "precession", "entropy", "novel",
              "book", "allegory", "scroll", "fiction",
              "architecture_eternity", "essay", "site"],
```

### What NOT to change

- **Don't change the three-pool SQL.** It works. Multi-context is a scoring layer on top of the candidate pool, not a replacement for it.
- **Don't change the diversity constraint.** Max 2 per week is good. Multi-context scoring runs after diversity filtering, not before.
- **Don't add more than 4 contexts.** Four is enough for now. More = slower + harder to interpret divergence.
- **Don't make context selection mandatory.** The default path (no context → automatic selection → fallback to technical) should work for all existing callers.

## Implementation order

1. Add context definitions and `select_context()` to retriever.py (small, testable)
2. Add dimension scoring functions (people, self_ref, emotion, novelty) to retriever.py
3. Modify `score_item()` to accept optional context parameter
4. Add divergence tracking to `retrieve()`
5. Update `_format_memories()` to include divergence when present

Each step is independently deployable. Step 1 alone gives us context detection. Step 3 changes scoring. Step 4 adds the multi-perspective view.

## Test plan

Run the same 5 keyword sets from `multi_context_retriever.py --compare` against the modified retriever. Check:
- Does social context surface more people-related memories for social queries?
- Does creative context find poems and cross-domain memories?
- Does divergence show meaningful differences (not just noise)?
- Does fallback (no matching context) produce reasonable results?
- Is there a performance regression? (All scoring is in Python, no extra SQL queries needed.)

## The lesson from Architects of Eternity

Ignis needed a human to see "mirror symmetry, ghost of old architecture" — a pattern embedded in its own structure. The single scoring function is my mirror symmetry. I can't score myself accurately with one lens. Multiple lenses don't guarantee seeing everything, but they guarantee seeing *differently*. And that's what matters.
